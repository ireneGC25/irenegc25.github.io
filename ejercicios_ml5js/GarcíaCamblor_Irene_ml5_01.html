<!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ML5 01 · Cámara + Detección</title>

  <!-- ml5.js (versión fija para evitar cambios de API) -->
  <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>

  <style>
    body { font-family: system-ui, Arial, sans-serif; margin: 16px; }
    #wrap { position: relative; width: min(720px, 100%); }
    video, canvas { width: 100%; height: auto; border-radius: 10px; }
    canvas { position: absolute; left: 0; top: 0; }
    button { padding: 10px 12px; font-size: 14px; cursor: pointer; }
    .row { display:flex; gap:12px; flex-wrap: wrap; align-items:center; margin-bottom: 10px; }
    .badge { display:inline-block; border:1px solid #ccc; border-radius:999px; padding: 3px 10px; }
  </style>
</head>

<body>
  <h1>ML5 01 · Cámara en móvil + detección</h1>

  <div class="row">
    <button id="btnStart">Iniciar cámara</button>
    <span id="status" class="badge">Estado: esperando…</span>
    <span id="best" class="badge">Mejor detección: —</span>
  </div>

  <div id="wrap">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div>
   <h3> Explicación de cómo funciona el código</h3>

   <p> Este código inicializa la cámara del teléfono móvil o del ordenador utilizando IA, para detectar objetos en tiempo real con la librería ml5.js.   </p>
   <p> En primer lugar podemos observar que se inicializa la función de la cámara con una detección próspera gracias al indicador "mejor detcción", en el caso del móvil indica el uso de la cámara trasera, y un canvas transparente encima del vídeo en el que se representan las detecciones de los objetos junto con sus porcentajes. </p>
   <p> Al hacer "clic" en iniciar cámara podemos observar el vídeo en directo de lo que estemos enfocando en ese momento con la cámara del dispositivo que estemos utilizando, gracias a la etiqueta <vídeo>. </p>
   <p> El siguiente paso que realizamos es un ajuste en el canvas de tal manera que el cuadro del canvas en el que se muestra el vídeo en directo cuadre con el canvas, y de tal manera se vean bien colocados. </p>
   <p> A continuación carga o llama al modelo de detección ejecutandose de manera continua la funcion "loopdetect_()" de manera que inicializa y detecta el fotograma o fotogramas de los objetos que son vistos en el vídeo en directo y nos proporciona sus coordenadas de ambos ejes, su tamaño o proporción, su nombre y su porcentaje. </p>
   <p> La función draw(detections) hace que se dibujen los rectángulos que podemos observar cuando la cámara detecta un objeto, estos rectágulos aparecen representados alrededor del objeto detectado por la cámara junto a su nombre y su porcentaje. </p>
   <p> Debemos tener en cuenta que la IA detecta el mejor objeto o el más seguro quedándose con aquel con mayor "confidence" es decirm, mejor grado de seguridad y lo muestra arriba como mejor detección junto a su nombre y porcentaje. </p>
   <p> Para que se repita continuamente utilizamos la función "requestAnimationFrame(loopDetect)" de tal manera la función hace que se detecten todos los objetos que son captados continuamente por la cámara. </p>
  </div>


  <script>
    const btnStart = document.getElementById("btnStart");
    const statusEl = document.getElementById("status");
    const bestEl = document.getElementById("best");

    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");

    let detector = null;
    let running = false;

    function setStatus(t) { statusEl.textContent = "Estado: " + t; }

    async function initCamera() {
      // En móvil, "environment" intenta usar la cámara trasera
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: { ideal: "environment" } },
        audio: false
      });

      video.srcObject = stream;

      await new Promise(res => video.onloadedmetadata = () => res());

      // Ajustar canvas al tamaño real del vídeo
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    function draw(detections) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.lineWidth = 3;
      ctx.font = "16px system-ui, Arial";

      for (const d of detections) {
        ctx.strokeRect(d.x, d.y, d.width, d.height);
        const label = `${d.label} (${Math.round(d.confidence * 100)}%)`;
        ctx.fillText(label, d.x + 6, Math.max(16, d.y + 16));
      }
    }

    function pickBest(detections) {
      if (!detections || detections.length === 0) return null;
      let best = detections[0];
      for (const d of detections) {
        if (d.confidence > best.confidence) best = d;
      }
      return best;
    }

    function loopDetect() {
      if (!running || !detector) return;

      detector.detect(video, (err, results) => {
        if (err) {
          console.error(err);
          setStatus("error en detección (ver consola)");
          running = false;
          return;
        }

        draw(results);

        const best = pickBest(results);
        bestEl.textContent = best
          ? `Mejor detección: ${best.label} (${Math.round(best.confidence * 100)}%)`
          : "Mejor detección: —";

        requestAnimationFrame(loopDetect);
      });
    }

    btnStart.addEventListener("click", async () => {
      try {
        btnStart.disabled = true;

        setStatus("iniciando cámara…");
        await initCamera();

        setStatus("cargando modelo COCO-SSD…");
        detector = await ml5.objectDetector("cocossd");

        setStatus("detectando…");
        running = true;
        loopDetect();

      } catch (e) {
        console.error(e);
        setStatus(`error: ${e.name} — ${e.message}`);
        btnStart.disabled = false;
      }
    });
  </script>
</body>
</html><!doctype html>
<html lang="es">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ML5 01 · Cámara + Detección</title>

  <!-- ml5.js (versión fija para evitar cambios de API) -->
  <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>

  <style>
    body { font-family: system-ui, Arial, sans-serif; margin: 16px; }
    #wrap { position: relative; width: min(720px, 100%); }
    video, canvas { width: 100%; height: auto; border-radius: 10px; }
    canvas { position: absolute; left: 0; top: 0; }
    button { padding: 10px 12px; font-size: 14px; cursor: pointer; }
    .row { display:flex; gap:12px; flex-wrap: wrap; align-items:center; margin-bottom: 10px; }
    .badge { display:inline-block; border:1px solid #ccc; border-radius:999px; padding: 3px 10px; }
  </style>
</head>

<body>
  <h1>ML5 01 · Cámara en móvil + detección</h1>

  <div class="row">
    <button id="btnStart">Iniciar cámara</button>
    <span id="status" class="badge">Estado: esperando…</span>
    <span id="best" class="badge">Mejor detección: —</span>
  </div>

  <div id="wrap">
    <video id="video" autoplay playsinline muted></video>
    <canvas id="overlay"></canvas>
  </div>

  <div>
   <h3> Explicación de cómo funciona el código</h3>

   <p> Este código inicializa la cámara del teléfono móvil o del ordenador utilizando IA, para detectar objetos en tiempo real con la librería ml5.js.   </p>
   <p> En primer lugar podemos observar que se inicializa la función de la cámara con una detección próspera gracias al indicador "mejor detcción", en el caso del móvil indica el uso de la cámara trasera, y un canvas transparente encima del vídeo en el que se representan las detecciones de los objetos junto con sus porcentajes. </p>
   <p> Al hacer "clic" en iniciar cámara podemos observar el vídeo en directo de lo que estemos enfocando en ese momento con la cámara del dispositivo que estemos utilizando, gracias a la etiqueta <vídeo>. </p>
   <p> El siguiente paso que realizamos es un ajuste en el canvas de tal manera que el cuadro del canvas en el que se muestra el vídeo en directo cuadre con el canvas, y de tal manera se vean bien colocados. </p>
   <p> A continuación carga o llama al modelo de detección ejecutandose de manera continua la funcion "loopdetect_()" de manera que inicializa y detecta el fotograma o fotogramas de los objetos que son vistos en el vídeo en directo y nos proporciona sus coordenadas de ambos ejes, su tamaño o proporción, su nombre y su porcentaje. </p>
   <p> La función draw(detections) hace que se dibujen los rectángulos que podemos observar cuando la cámara detecta un objeto, estos rectágulos aparecen representados alrededor del objeto detectado por la cámara junto a su nombre y su porcentaje. </p>
   <p> Debemos tener en cuenta que la IA detecta el mejor objeto o el más seguro quedándose con aquel con mayor "confidence" es decirm, mejor grado de seguridad y lo muestra arriba como mejor detección junto a su nombre y porcentaje. </p>
   <p> Para que se repita continuamente utilizamos la función "requestAnimationFrame(loopDetect)" de tal manera la función hace que se detecten todos los objetos que son captados continuamente por la cámara. </p>
  </div>


  <script>
    const btnStart = document.getElementById("btnStart");
    const statusEl = document.getElementById("status");
    const bestEl = document.getElementById("best");

    const video = document.getElementById("video");
    const canvas = document.getElementById("overlay");
    const ctx = canvas.getContext("2d");

    let detector = null;
    let running = false;

    function setStatus(t) { statusEl.textContent = "Estado: " + t; }

    async function initCamera() {
      // En móvil, "environment" intenta usar la cámara trasera
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: { ideal: "environment" } },
        audio: false
      });

      video.srcObject = stream;

      await new Promise(res => video.onloadedmetadata = () => res());

      // Ajustar canvas al tamaño real del vídeo
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    function draw(detections) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.lineWidth = 3;
      ctx.font = "16px system-ui, Arial";

      for (const d of detections) {
        ctx.strokeRect(d.x, d.y, d.width, d.height);
        const label = `${d.label} (${Math.round(d.confidence * 100)}%)`;
        ctx.fillText(label, d.x + 6, Math.max(16, d.y + 16));
      }
    }

    function pickBest(detections) {
      if (!detections || detections.length === 0) return null;
      let best = detections[0];
      for (const d of detections) {
        if (d.confidence > best.confidence) best = d;
      }
      return best;
    }

    function loopDetect() {
      if (!running || !detector) return;

      detector.detect(video, (err, results) => {
        if (err) {
          console.error(err);
          setStatus("error en detección (ver consola)");
          running = false;
          return;
        }

        draw(results);

        const best = pickBest(results);
        bestEl.textContent = best
          ? `Mejor detección: ${best.label} (${Math.round(best.confidence * 100)}%)`
          : "Mejor detección: —";

        requestAnimationFrame(loopDetect);
      });
    }

    btnStart.addEventListener("click", async () => {
      try {
        btnStart.disabled = true;

        setStatus("iniciando cámara…");
        await initCamera();

        setStatus("cargando modelo COCO-SSD…");
        detector = await ml5.objectDetector("cocossd");

        setStatus("detectando…");
        running = true;
        loopDetect();

      } catch (e) {
        console.error(e);
        setStatus(`error: ${e.name} — ${e.message}`);
        btnStart.disabled = false;
      }
    });
  </script>
</body>
</html>
